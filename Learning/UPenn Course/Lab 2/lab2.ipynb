{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbm(n, c, p_intra, p_inter):\n",
    "    \n",
    "    # assign a community to each node\n",
    "    community = np.repeat(list(range(c)), np.ceil(n / c))\n",
    "\n",
    "    # make sure community vector has size n\n",
    "    community = community[0:n]\n",
    "\n",
    "    # make it a column vector\n",
    "    community = np.expand_dims(community, 1)\n",
    "\n",
    "\n",
    "    # generate a boolean matrix indicating whether two nodes \n",
    "    # are in the same community\n",
    "    intra = community == community.T\n",
    "\n",
    "    # generate a boolean matrix indicating whether two nodes \n",
    "    # are in different communities\n",
    "    inter = np.logical_not(intra)\n",
    "\n",
    "    # generate a matrix with random entries between 0 and 1\n",
    "    random = np.random.random((n, n))\n",
    "\n",
    "    # generate a triangular matrix with zeros below the main diagonal\n",
    "    # because the SBM graph is symmetric, we only have to assign weights \n",
    "    # to the upper triangular part of the adjacency matrix,\n",
    "    # and later copy them to the lower triangular part\n",
    "    tri = np.tri(n, k=-1)\n",
    "\n",
    "\n",
    "    # initialize adjacency matrix\n",
    "    graph = np.zeros((n, n))\n",
    "\n",
    "    # assign intra-community edges\n",
    "    graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1\n",
    "\n",
    "    # assign inter-community edges\n",
    "    graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1\n",
    "\n",
    "    # make the adjacency matrix symmetric\n",
    "    graph += graph.T \n",
    "\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sbm(n=50, c=5, p_intra=0.6, p_inter=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gso(gso):\n",
    "    \n",
    "    # obtain eigenvalues\n",
    "    eigenvalues, _ = np.linalg.eig(gso) \n",
    "\n",
    "    # normalize by eigenvalue with largest absolute value\n",
    "    return gso / np.max(np.abs(eigenvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = normalize_gso(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3 Data Generation and Training/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffusion(gso, n_samples, n_sources):\n",
    "\n",
    "    # get the number of nodes\n",
    "    n = gso.shape[0]\n",
    "\n",
    "    # initialize the tensor used to store the samples\n",
    "    # shape is n_samples x n x time x 1 features\n",
    "    z = np.zeros((n_samples, n, 5, 1))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # pick n_sources at random from n nodes\n",
    "        sources = np.random.choice(n, n_sources, replace=False)\n",
    "\n",
    "        # define z_0 for each sample\n",
    "        z[i, sources, 0, 0] = np.random.uniform(0,10, n_sources)\n",
    "\n",
    "    # noise mean and variance\n",
    "    mu = np.zeros(n)\n",
    "    sigma = np.eye(n) * 1e-3\n",
    "\n",
    "    for t in range(4):\n",
    "\n",
    "        # generate noise\n",
    "        noise = np.random.multivariate_normal(mu, sigma, n_samples)\n",
    "\n",
    "        # generate z_t\n",
    "        z[:, :, t + 1] = gso @ z[:, :, t] + np.expand_dims(noise, -1)\n",
    "        \n",
    "    # transpose dimensions so shape is n_samples x time x n x 1 feature\n",
    "    z = z.transpose((0, 2, 1, 3))\n",
    "    \n",
    "    # squeeze feature dimension, as there is only 1 feature\n",
    "    return z.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = generate_diffusion(S, 2100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_diffusion(z):\n",
    "    \n",
    "    # permute the samples in z\n",
    "    z = np.random.permutation(z)\n",
    "    \n",
    "    # define the output tensor\n",
    "    y = np.expand_dims(z[:, 0, :], 1)\n",
    "    \n",
    "    # initialize the input tensor\n",
    "    x = np.zeros(y.shape)\n",
    "    \n",
    "    # define the input tensor as x = z_4\n",
    "    for i, sample in enumerate(z):\n",
    "        x[i] = sample[4]\n",
    "   \n",
    "    # squeeze time dimension     \n",
    "    return x.squeeze(), y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, splits=(2000, 100)):\n",
    "\n",
    "    # define the initial index of each set (training/test)\n",
    "    splits = np.cumsum([0] + list(splits))\n",
    "    splits = (splits * x.shape[0] / splits[-1]).astype(int)\n",
    "\n",
    "    # return training and test data as tuples\n",
    "    return ((x[splits[i]:splits[i + 1]], y[splits[i]:splits[i + 1]]) for i in range(len(splits) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data_from_diffusion(z)\n",
    "trainData, testData = split_data(x, y, (2000,100))\n",
    "xTrain = trainData[0]\n",
    "yTrain = trainData[1]\n",
    "xTest = testData[0]\n",
    "yTest = testData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = torch.tensor(xTrain)\n",
    "yTrain = torch.tensor(yTrain)\n",
    "xTest = torch.tensor((xTest))\n",
    "yTest = torch.tensor(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Graph Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterFunction(h, S, x):\n",
    "\n",
    "    K = h.shape[0] # number of filter taps\n",
    "    B = x.shape[0] # batch size\n",
    "    N = x.shape[1] # number of nodes\n",
    "\n",
    "    x = x.reshape([B, 1, N])\n",
    "    S = S.reshape([1, N, N])\n",
    "    z = x\n",
    "\n",
    "    for k in range(1, K):\n",
    "\n",
    "        # diffusion step, S^k*x\n",
    "        x = torch.matmul(x, S)\n",
    "        xS = x.reshape([B, 1, N]) \n",
    "\n",
    "        # concatenate the S^k*x in the tensor z\n",
    "        z = torch.cat((z, xS), dim=1) \n",
    "\n",
    "    # multiply z and h in the concatenation dimension\n",
    "    y = torch.matmul(z.permute(0, 2, 1).reshape([B, N, K]), h)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFilter(nn.Module):\n",
    "    def __init__(self, gso, k):\n",
    "        \n",
    "        # Initialize parent\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save filter hyperparameters\n",
    "        self.gso = torch.tensor(gso)\n",
    "        self.n = gso.shape[0]\n",
    "        self.k = k\n",
    "        \n",
    "        # Define and initialize learnable weights\n",
    "        self.weight = nn.Parameter(torch.randn(self.k))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.k)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return FilterFunction(self.weight, self.gso, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphFilter = GraphFilter(S, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphPerceptron(nn.Module):\n",
    "    def __init__(self, gso, k, sigma):\n",
    "        super().__init__()\n",
    "        self.gso = torch.tensor(gso)\n",
    "        self.n = gso.shape[0]\n",
    "        self.k = k\n",
    "        self.sigma = sigma\n",
    "        self.weight = nn.Parameter(torch.randn(self.k))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.k)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = FilterFunction(self.weight, self.gso, x)\n",
    "        y = self.sigma(y)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphPerceptron = GraphPerceptron(S, 8, nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLGNN(nn.Module):\n",
    "    def __init__(self, gso, l, k, sigma):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for layer in range(l):\n",
    "            layers.append(GraphPerceptron(gso, k[layer], sigma))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLGNN = MLGNN(S, 2, [8, 1], nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the Filter function\n",
    "def FilterFunction(h, S, x):\n",
    "    \n",
    "    # Number of output features\n",
    "    F = h.shape[0]\n",
    "    \n",
    "    # Number of filter taps\n",
    "    K = h.shape[1]\n",
    "    \n",
    "    # Number of input features\n",
    "    G = h.shape[2]\n",
    "    \n",
    "    # Number of nodes\n",
    "    N = S.shape[1]\n",
    "    \n",
    "    # Batch size\n",
    "    B = x.shape[0]\n",
    "\n",
    "    # Create concatenation dimension and initialize concatenation tensor z\n",
    "    x = x.reshape([B, 1, G, N])\n",
    "    S = S.reshape([1, N, N])\n",
    "    z = x\n",
    "    \n",
    "    # Loop over the number of filter taps\n",
    "    for k in range(1, K):\n",
    "        \n",
    "        # S*x\n",
    "        x = torch.matmul(x, S)\n",
    "        \n",
    "        # Reshape\n",
    "        xS = x.reshape([B, 1, G, N])\n",
    "        \n",
    "        # Concatenate\n",
    "        z = torch.cat((z, xS), dim=1)\n",
    "    \n",
    "    # Multiply by h\n",
    "    y = torch.matmul(z.permute(0, 3, 1, 2).reshape([B, N, K*G]), \n",
    "                     h.reshape([F, K*G]).permute(1, 0)).permute(0, 2, 1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the graph filter class\n",
    "class GraphFilter(nn.Module):\n",
    "    def __init__(self, gso, k, f_in, f_out):\n",
    "        super().__init__()\n",
    "        self.gso = torch.tensor(gso)\n",
    "        self.n = gso.shape[0]\n",
    "        self.k = k\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        self.weight = nn.Parameter(torch.randn(self.f_out, self.k, self.f_in))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.f_in * self.k)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return FilterFunction(self.weight, self.gso, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m GraphFilter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(GraphFilter(gso\u001b[38;5;241m=\u001b[39mS, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, f_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, f_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m),GraphFilter(gso\u001b[38;5;241m=\u001b[39m\u001b[43mG\u001b[49m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, f_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, f_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "GraphFilter = torch.nn.Sequential(GraphFilter(gso=S, k=8, f_in=1, f_out=32),GraphFilter(gso=G, k=1, f_in=32, f_out=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

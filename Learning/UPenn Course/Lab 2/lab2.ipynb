{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbm(n, c, p_intra, p_inter):\n",
    "    \n",
    "    # assign a community to each node\n",
    "    community = np.repeat(list(range(c)), np.ceil(n / c))\n",
    "\n",
    "    # make sure community vector has size n\n",
    "    community = community[0:n]\n",
    "\n",
    "    # make it a column vector\n",
    "    community = np.expand_dims(community, 1)\n",
    "\n",
    "\n",
    "    # generate a boolean matrix indicating whether two nodes \n",
    "    # are in the same community\n",
    "    intra = community == community.T\n",
    "\n",
    "    # generate a boolean matrix indicating whether two nodes \n",
    "    # are in different communities\n",
    "    inter = np.logical_not(intra)\n",
    "\n",
    "    # generate a matrix with random entries between 0 and 1\n",
    "    random = np.random.random((n, n))\n",
    "\n",
    "    # generate a triangular matrix with zeros below the main diagonal\n",
    "    # because the SBM graph is symmetric, we only have to assign weights \n",
    "    # to the upper triangular part of the adjacency matrix,\n",
    "    # and later copy them to the lower triangular part\n",
    "    tri = np.tri(n, k=-1)\n",
    "\n",
    "\n",
    "    # initialize adjacency matrix\n",
    "    graph = np.zeros((n, n))\n",
    "\n",
    "    # assign intra-community edges\n",
    "    graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1\n",
    "\n",
    "    # assign inter-community edges\n",
    "    graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1\n",
    "\n",
    "    # make the adjacency matrix symmetric\n",
    "    graph += graph.T \n",
    "\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sbm(n=50, c=5, p_intra=0.6, p_inter=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gso(gso):\n",
    "    \n",
    "    # obtain eigenvalues\n",
    "    eigenvalues, _ = np.linalg.eig(gso) \n",
    "\n",
    "    # normalize by eigenvalue with largest absolute value\n",
    "    return gso / np.max(np.abs(eigenvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = normalize_gso(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3 Data Generation and Training/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffusion(gso, n_samples, n_sources):\n",
    "\n",
    "    # get the number of nodes\n",
    "    n = gso.shape[0]\n",
    "\n",
    "    # initialize the tensor used to store the samples\n",
    "    # shape is n_samples x n x time x 1 features\n",
    "    z = np.zeros((n_samples, n, 5, 1))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # pick n_sources at random from n nodes\n",
    "        sources = np.random.choice(n, n_sources, replace=False)\n",
    "\n",
    "        # define z_0 for each sample\n",
    "        z[i, sources, 0, 0] = np.random.uniform(0,10, n_sources)\n",
    "\n",
    "    # noise mean and variance\n",
    "    mu = np.zeros(n)\n",
    "    sigma = np.eye(n) * 1e-3\n",
    "\n",
    "    for t in range(4):\n",
    "\n",
    "        # generate noise\n",
    "        noise = np.random.multivariate_normal(mu, sigma, n_samples)\n",
    "\n",
    "        # generate z_t\n",
    "        z[:, :, t + 1] = gso @ z[:, :, t] + np.expand_dims(noise, -1)\n",
    "        \n",
    "    # transpose dimensions so shape is n_samples x time x n x 1 feature\n",
    "    z = z.transpose((0, 2, 1, 3))\n",
    "    \n",
    "    # squeeze feature dimension, as there is only 1 feature\n",
    "    return z.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = generate_diffusion(S, 2100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_diffusion(z):\n",
    "    \n",
    "    # permute the samples in z\n",
    "    z = np.random.permutation(z)\n",
    "    \n",
    "    # define the output tensor\n",
    "    y = np.expand_dims(z[:, 0, :], 1)\n",
    "    \n",
    "    # initialize the input tensor\n",
    "    x = np.zeros(y.shape)\n",
    "    \n",
    "    # define the input tensor as x = z_4\n",
    "    for i, sample in enumerate(z):\n",
    "        x[i] = sample[4]\n",
    "   \n",
    "    # squeeze time dimension     \n",
    "    return x.squeeze(), y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, splits=(2000, 100)):\n",
    "\n",
    "    # define the initial index of each set (training/test)\n",
    "    splits = np.cumsum([0] + list(splits))\n",
    "    splits = (splits * x.shape[0] / splits[-1]).astype(int)\n",
    "\n",
    "    # return training and test data as tuples\n",
    "    return ((x[splits[i]:splits[i + 1]], y[splits[i]:splits[i + 1]]) for i in range(len(splits) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data_from_diffusion(z)\n",
    "trainData, testData = split_data(x, y, (2000,100))\n",
    "xTrain = trainData[0]\n",
    "yTrain = trainData[1]\n",
    "xTest = testData[0]\n",
    "yTest = testData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = torch.tensor(xTrain)\n",
    "yTrain = torch.tensor(yTrain)\n",
    "xTest = torch.tensor((xTest))\n",
    "yTest = torch.tensor(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Graph Filters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
